{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8207310,"sourceType":"datasetVersion","datasetId":4863207},{"sourceId":8359200,"sourceType":"datasetVersion","datasetId":4967682},{"sourceId":8507305,"sourceType":"datasetVersion","datasetId":5078021},{"sourceId":8508811,"sourceType":"datasetVersion","datasetId":5079099},{"sourceId":8516689,"sourceType":"datasetVersion","datasetId":5084773},{"sourceId":8521585,"sourceType":"datasetVersion","datasetId":5088178},{"sourceId":9261593,"sourceType":"datasetVersion","datasetId":5604066},{"sourceId":176224317,"sourceType":"kernelVersion"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\nimport sys\nsys.path.insert(0,'..')\nfrom pathlib import Path\nimport os\nimport warnings\n\nimport random\nimport numpy as np\nimport collections\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nimport joblib\nfrom textwrap import dedent\nimport time\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.manifold import TSNE\n\nfrom focal_loss import FocalLoss\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset, SubsetRandomSampler\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package],\n                         stdout=subprocess.DEVNULL,\n                         stderr=subprocess.DEVNULL\n                         )\nrequired_packages = [\n    \"torchsummary\",\n]\nfor package in required_packages:\n    try:\n        __import__(package)\n        print(f\"{package} is already installed.\")\n    except ImportError:\n        print(f\"{package} is not installed. Installing...\")\n        install(package)\nfrom torchsummary import summary\nimport torch.optim.lr_scheduler as lr_scheduler","metadata":{"_cell_guid":"b7a7e980-0ab5-459c-99ea-96a0d0581546","_uuid":"69aa9580-c39d-419c-8046-d4a239b5b607","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:02.104790Z","iopub.execute_input":"2024-08-28T23:40:02.105209Z","iopub.status.idle":"2024-08-28T23:40:05.749873Z","shell.execute_reply.started":"2024-08-28T23:40:02.105170Z","shell.execute_reply":"2024-08-28T23:40:05.748737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Flags and Variables","metadata":{"_cell_guid":"68e824bb-275c-4f4c-a7be-760795f10713","_uuid":"e6264650-af66-4841-a683-d4bdacad4d28","trusted":true}},{"cell_type":"code","source":"# set paths\n\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    # Kaggle environment\n    root_dir = '/kaggle/input/pre-processed-data'\nelif 'COLAB_GPU' in os.environ:\n    # Google Colab environment\n    root_dir = '/content/drive/MyDrive/your-dataset-directory'\nelse:\n    # Local environment\n    root_dir = r'C:\\Users\\Khizer Zakir\\OneDrive - Universit√© Bretagne Sud\\thesis\\Covariance_Thesis\\notebooks\\npz_data_preprocessed'\nnpz_file = \"data_preprocessed.npz\"\n\nfile_path = os.path.join(root_dir, npz_file)\n\nmodel_name = \"CNN\"   # Change to \"MLP\", \"LSTM\", or \"CNN\" as needed\nfor model_name in [\"MLP\", \"LSTM\", \"CNN\"]:\n    folder_name = f\"{model_name}_FR\"\n    os.makedirs(folder_name, exist_ok=True)\n    print(f\"Folder '{folder_name}' created or already exists.\")\n    \nos.makedirs(folder_name, exist_ok=True)\nbatch_size = 64  # batch size\nn_classes = 11\ndropout_rate = 0\nrandom_seed = 42\ntest_size = 0.2\nval_size = 0.5\nval = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfigure_name = f\"CM_{model_name}\"\nplot_figure_name = f\"training_validation_results_{model_name}\"\n\n# for model\nlr=1e-2\nepochs=200\ngamma_value=0.95\nloss = \"focal\"\ngamma_loss = 3\nstep_size=30\nstep_lr = False","metadata":{"_cell_guid":"6ea9e5c9-a88f-4557-95c6-4b7134e77f19","_uuid":"b6220987-eaa9-45a2-815c-b260081212a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:05.751652Z","iopub.execute_input":"2024-08-28T23:40:05.752137Z","iopub.status.idle":"2024-08-28T23:40:05.786776Z","shell.execute_reply.started":"2024-08-28T23:40:05.752100Z","shell.execute_reply":"2024-08-28T23:40:05.785842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model and Dataset definition","metadata":{"_cell_guid":"58850a6b-0923-4040-8981-4e3c19d4632b","_uuid":"5a178b9c-9c01-44de-b4d7-8a2409ffb8ed","trusted":true}},{"cell_type":"code","source":"class TimeSeriesMLP(nn.Module):\n\n    def __init__(self, input_size, output_size, hidden_units=[128, 256, 128], dropout_rate=None):\n        super(TimeSeriesMLP, self).__init__()\n\n        self.fc_in = nn.Linear(input_size, hidden_units[0])\n        fc_modules = []\n\n        for i in range(len(hidden_units)-1):\n            fc_modules.append(nn.Linear(hidden_units[i], hidden_units[i+1]))\n            fc_modules.append(nn.ReLU())\n\n            if dropout_rate is not None:\n                fc_modules.append(nn.Dropout(dropout_rate))  # Add dropout after ReLU\n\n        self.fc_hidden = nn.Sequential(*fc_modules)\n        self.fc_out = nn.Linear(hidden_units[-1], output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc_in(x))\n        x = self.fc_hidden(x)\n        x = self.softmax(self.fc_out(x))\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T23:40:05.788000Z","iopub.execute_input":"2024-08-28T23:40:05.788416Z","iopub.status.idle":"2024-08-28T23:40:05.797842Z","shell.execute_reply.started":"2024-08-28T23:40:05.788371Z","shell.execute_reply":"2024-08-28T23:40:05.796841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeSeriesLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n        super(TimeSeriesLSTM, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        x = self.softmax(self.fc(x[:, -1, :]))  # Use the last time step's output for prediction\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-08-28T23:40:05.799974Z","iopub.execute_input":"2024-08-28T23:40:05.800316Z","iopub.status.idle":"2024-08-28T23:40:05.811580Z","shell.execute_reply.started":"2024-08-28T23:40:05.800281Z","shell.execute_reply":"2024-08-28T23:40:05.810786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# class CNN1D(nn.Module):\n#     def __init__(self, input_channels, num_classes, kernel_size=3, dropout_prob=0):\n#         super(CNN1D, self).__init__()\n#         self.conv_block1 = self.create_conv_block(input_channels, 32, kernel_size, dropout_prob)\n#         self.conv_block2 = self.create_conv_block(32, 64, kernel_size, dropout_prob)\n#         self.conv_block3 = self.create_conv_block(64, 128, kernel_size, dropout_prob)\n#         self.fc1 = nn.Linear(128 * 1, 64)\n#         self.fc2 = nn.Linear(64, num_classes)\n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(dropout_prob)\n#         self.softmax =  nn.Softmax(dim=1)\n\n#     def create_conv_block(self, in_channels, out_channels, kernel_size, dropout_prob):\n#         return nn.Sequential(\n#             nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1),\n#             nn.BatchNorm1d(out_channels),\n#             nn.ReLU(),\n#             nn.Dropout(dropout_prob),\n#             nn.MaxPool1d(kernel_size=3)\n#         )\n\n#     def forward(self, x):\n#         x = x.permute(0, 2, 1)\n#         x = self.conv_block1(x)\n#         x = self.conv_block2(x)\n#         x = self.conv_block3(x)\n#         x = x.view(x.size(0), -1)  # Flatten the tensor\n#         x = self.fc1(x)\n#         x = self.relu(x)\n#         x = self.dropout(x)\n#         x = self.softmax(self.fc2(x))\n#         return x\n    \nclass CNN1D(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(CNN1D, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=3)\n        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.fc1 = nn.Linear(64 , 32) \n        self.fc2 = nn.Linear(32, num_classes)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = x.permute(0, 2, 1)  # Permute to [m, feature, sequence] for Conv1d\n        x = self.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.relu(self.fc1(x))\n        x = self.softmax(self.fc2(x))  # Using softmax for multiclass classification\n        return x","metadata":{"_cell_guid":"70f813ec-9928-4dac-a03b-27eb7c350390","_uuid":"558cf6a6-ff6c-4305-a256-49fb91eb1b91","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:05.812918Z","iopub.execute_input":"2024-08-28T23:40:05.813219Z","iopub.status.idle":"2024-08-28T23:40:05.823346Z","shell.execute_reply.started":"2024-08-28T23:40:05.813188Z","shell.execute_reply":"2024-08-28T23:40:05.822568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset and transforms\n\nclass NormalizeTransform:\n    def __init__(self, data):\n        self.percentile_98 = np.percentile(data, 98)\n        # make it torch tensor\n        self.percentile_98 = torch.tensor(self.percentile_98, dtype=torch.float32)\n    def __call__(self, sample):\n        return sample / self.percentile_98\n\nclass data_CNN(Dataset):\n    def __init__(self, root_dir, npz_file, transform=None):\n        self.root_dir = root_dir\n        self.npz_file = npz_file\n        self.x, self.y, self.class_names = self.load_data()\n        \n        if transform is None:\n            self.transform = NormalizeTransform(self.x)\n        else:\n            self.transform = transform\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        sample = self.x[idx]\n        sample = torch.tensor(sample, dtype=torch.float32)\n        label = self.y[idx]\n        label = torch.tensor(label, dtype=torch.uint8)\n        class_name = self.class_names[label]\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample, label\n\n    def load_data(self):\n        file_path = os.path.join(self.root_dir, self.npz_file)\n        data = np.load(file_path, allow_pickle=True)\n        x = data['series']\n        y = data['labels']\n        class_names = data['class_names']\n        return x, y, class_names","metadata":{"_cell_guid":"92352a0f-6f88-46d7-a850-d70e8e2bde24","_uuid":"43d06680-df17-4a4c-869a-ce5235cc3891","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:05.824385Z","iopub.execute_input":"2024-08-28T23:40:05.824648Z","iopub.status.idle":"2024-08-28T23:40:05.837918Z","shell.execute_reply.started":"2024-08-28T23:40:05.824620Z","shell.execute_reply":"2024-08-28T23:40:05.837059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions","metadata":{"_cell_guid":"1fc878b3-3e05-42ba-bbb9-d9f207591027","_uuid":"bd7d77d4-3d74-4cbb-9d46-dc2be86556be","trusted":true}},{"cell_type":"code","source":"def initialize_model(model_name, input_size, n_classes=None, hidden_units=None, hidden_size=None, num_layers=1, dropout_rate=None):\n    if model_name == \"TimeSeriesMLP\":\n        if hidden_units is None:\n            hidden_units = [128, 256, 128]\n        model = TimeSeriesMLP(input_size=input_size, output_size=n_classes, hidden_units=hidden_units, dropout_rate=dropout_rate)\n        print(\"TimeSeriesMLP model initialized\")\n\n    elif model_name == \"TimeSeriesLSTM\":\n        if hidden_size is None:\n            hidden_size = 128\n        model = TimeSeriesLSTM(input_size=input_size, hidden_size=hidden_size, output_size=n_classes, num_layers=num_layers)\n        print(\"TimeSeriesLSTM model initialized\")\n\n    elif model_name == \"CNN1D\":\n        if n_classes is None:\n            raise ValueError(\"Number of classes (n_classes) must be specified for CNN1D.\")\n#         model = CNN1D(input_channels=input_size, num_classes=n_classes, dropout_prob=dropout_rate)\n        model = CNN1D(input_size, n_classes)\n        print(model)\n        print(\"CNN1D model initialized\")\n\n    else:\n        raise ValueError(f\"Model {model_name} not found\")\n\n    # Print trainable parameters\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f'Total trainable parameters: {total_params}')\n    \n    return model\n\n\n# calculate the weights for the classes\ndef calculate_class_weights(labels):\n    class_sample_count = np.array([len(np.where(labels == t)[0]) for t in np.unique(labels)])\n    weight = 1 - class_sample_count / len(labels)\n    return torch.from_numpy(weight).double()\n# data loaders\ndef create_data_loaders(dataset, train_indices, val_indices, test_indices, batch_size, val=True):\n    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices), shuffle=False)\n    \n    if val:\n        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_indices))\n        return train_loader, val_loader, test_loader\n    else:\n        return train_loader, test_loader\n    \n# plotting data distribution\ndef plot_class_distribution(train_loader, val_loader, test_loader, n_classes):\n\n    class_names = train_loader.dataset.class_names\n    \n    def count_labels(loader, n_classes):\n        counts = np.zeros(n_classes)\n        for _, labels in loader:\n            for l in labels:\n                counts[l] += 1\n        return counts\n\n    train_counts = count_labels(train_loader, n_classes)\n    \n    if val_loader is not None:\n        val_counts = count_labels(val_loader, n_classes)\n    else:\n        val_counts = np.zeros(n_classes)\n    \n    test_counts = count_labels(test_loader, n_classes)\n\n    # Bar plot\n    plt.figure(figsize=(12, 6))\n    plt.bar(np.arange(n_classes) - test_size, train_counts, 0.2, label='Train')\n    if val_loader is not None:\n        plt.bar(np.arange(n_classes), val_counts, 0.2, label='Validation')\n    plt.bar(np.arange(n_classes) + 0.2, test_counts, 0.2, label='Test')\n    plt.xlabel('Classes')\n    plt.ylabel('Count')\n    plt.yscale('log')\n    if val_loader is not None:\n        plt.title('Class Distribution in Train, Validation, and Test Sets')\n    else:\n        plt.title('Class Distribution in Train and Test Sets')\n    plt.xticks(np.arange(n_classes), class_names, rotation=45)\n    plt.legend(loc='best')\n    plt.show()\n\n    plt.figure(figsize=(12, 6))\n    plt.bar(np.arange(n_classes), train_counts, alpha=0.5, label='Train')\n    plt.bar(np.arange(n_classes), val_counts, alpha=0.5, label='Validation')\n    plt.bar(np.arange(n_classes), test_counts, alpha=0.5, label='Test')\n    plt.xlabel('Classes')\n    plt.ylabel('Count')\n    plt.yscale('log')\n    if val_loader is not None:\n        plt.title('Histogram of Class Distribution in Train, Validation, and Test Sets')\n    else:\n        plt.title('Histogram of Class Distribution in Train and Test Sets')\n    plt.xticks(np.arange(n_classes), class_names, rotation=45)\n    plt.legend(loc='best')\n    plt.show()","metadata":{"_cell_guid":"e8384eb2-69d0-4de1-b340-2565e8be823a","_uuid":"5b893a24-be20-4bb7-a3cf-23846f913e65","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:41:27.252949Z","iopub.execute_input":"2024-08-28T23:41:27.253389Z","iopub.status.idle":"2024-08-28T23:41:27.274090Z","shell.execute_reply.started":"2024-08-28T23:41:27.253350Z","shell.execute_reply":"2024-08-28T23:41:27.273087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train_one_epoch(model, train_loader, loss_fn, optimizer, device):\n    model.train()\n    loss_train, acc_train = [], []\n    \n    for x,y in train_loader:\n        x = x.to(device)\n        y = y.to(device).long()\n        optimizer.zero_grad()\n        # Forward pass\n        out = model(x)\n        l = loss_fn(out, y)\n        \n        acc, loss = (out.argmax(1) == y).cpu().numpy().sum() / out.shape[0], l.cpu().data.numpy()\n        loss_train.append(loss)\n        acc_train.append(acc)\n        l.backward()\n        optimizer.step()\n\n    acc_train = np.asarray(acc_train).mean()\n    loss_train = np.asarray(loss_train).mean()\n    \n    return loss_train, acc_train\n\ndef validate_one_epoch(model, val_loader, loss_fn, device):\n    model.eval()\n    loss_val, acc_val_list = [], []\n    y_true, y_pred = [], []\n\n    for x,y in val_loader:\n        x = x.to(device)\n        y = y.to(device).long()\n        \n        with torch.no_grad():\n            # Forward pass\n            out = model(x)\n            l = loss_fn(out, y)\n        \n        loss_val.append(l.cpu().data.numpy())\n        predicted_labels = out.argmax(1)\n        y_true.extend(list(y.cpu().numpy()))\n        y_pred.extend(list(predicted_labels.cpu().numpy()))\n        acc = (predicted_labels == y).cpu().numpy().sum() / out.shape[0]\n        acc_val_list.append(acc)\n    \n    acc_val = np.asarray(acc_val_list).mean()\n    loss_val = np.asarray(loss_val).mean()\n    f1_test = f1_score(y_true, y_pred, average='macro')\n    \n    return loss_val, acc_val, f1_test, y_true, y_pred\n\ndef train_model(model, train_loader, val_loader, loss_fn, optimizer, scheduler, epochs, folder_name):\n    t1, t2, t3, t4, t5 = [], [], [], [], []\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    min_val_loss = float('inf')\n    best_epoch = 0\n\n    for epoch in range(epochs):\n        start_time = time.time()  # Start time of the epoch\n\n        # Training phase\n        loss_train, acc_train = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n        scheduler.step()\n\n        t1.append(loss_train)\n        t2.append(100 * acc_train)\n\n        # Validation phase\n        loss_val, acc_val, f1_test, y_true, y_pred = validate_one_epoch(model, val_loader, loss_fn, device)\n\n       \n        t3.append(loss_val)\n        t4.append(100 * acc_val)\n        t5.append(100 * f1_test)\n        \n        print('Epoch {}/{} - Train loss: {:.4f} - Val loss: {:.4f} - Train acc: {:.2f}% - Val acc: {:.2f}% - Test F1-score: {:.2f}'.format(\n            epoch + 1, epochs, loss_train, loss_val, 100 * acc_train, 100 * acc_val, 100 * f1_test))\n\n        elapsed_time = time.time() - start_time\n        print('Elapsed time: {:.2f} seconds'.format(elapsed_time))\n        \n        # Save the best model\n        if loss_val < min_val_loss:\n            min_val_loss = loss_val\n            best_epoch = epoch + 1\n\n            ckpt = {\n                'epoch': best_epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n                'train_loss_history': t1,\n                'train_accuracy_history': t2,\n                'val_loss_history': t3,\n                'val_accuracy_history': t4,\n                'y_true': y_true,\n                'y_pred': y_pred,\n                'loss': min_val_loss\n            }\n            \n            best_model_path = os.path.join(folder_name, 'best_model.pth')\n            torch.save(ckpt, best_model_path)\n            print(f'Best model checkpoint saved at {best_model_path}')\n\n    # Save training results to a CSV file\n    results_df = pd.DataFrame({\n        'epoch': np.arange(1, epochs + 1),\n        'train_loss': t1,\n        'train_acc': t2,\n        'val_loss': t3,\n        'val_acc': t4,\n        'f1_score': t5\n\n    })\n    results_df.to_csv(os.path.join(folder_name, 'training_results.csv'), index=False)\n\n    return t1, t2, t3, t4, t5, best_epoch, best_model_path","metadata":{"_cell_guid":"4b114e16-e64f-498a-806c-31ba48bed45a","_uuid":"d6d36c6f-49cb-4301-8706-bf8b889291f7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:05.862365Z","iopub.execute_input":"2024-08-28T23:40:05.862678Z","iopub.status.idle":"2024-08-28T23:40:06.027663Z","shell.execute_reply.started":"2024-08-28T23:40:05.862647Z","shell.execute_reply":"2024-08-28T23:40:06.026735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef plot_training_results(t1, t2, t3, t4, t5, folder_name, figure_name, best_epoch):\n    print(f'Best epoch: {best_epoch}')\n    plt.figure(figsize=(12, 8))\n    plt.title(\"Training and Validation Loss and Accuracy\")\n\n    # Plot training and validation loss\n    p1, = plt.plot(t1, label='Train loss', color='blue')\n    p2, = plt.plot(t3, label='Validation loss', color='red')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.ylim(0, 0.5)\n\n    ax2 = plt.twinx()\n\n    # Plot training and validation accuracy\n    p3, = ax2.plot(t2, label='Train accuracy', color='orange')\n    p4, = ax2.plot(t4, label='Validation accuracy', color='green')\n    ax2.set_ylabel('Accuracy (%)')\n\n    # Plot test F1-score\n    p5, = ax2.plot(t5, label='F1-score-Validation', linestyle='dashed', color='purple')\n\n    # Combine legends\n    lines = [p1, p2, p3, p4, p5]\n    labels = [line.get_label() for line in lines]\n    plt.legend(lines, labels, loc='center left', bbox_to_anchor=(1.1, 0.5))\n\n    # Save the figure with the provided name\n    plot_name = os.path.join(folder_name, figure_name)\n    plt.savefig(f'{plot_name}.png', bbox_inches='tight', pad_inches=0.2, dpi=500)\n    print(f'Figure saved as {figure_name}.png')\n    plt.show()","metadata":{"_cell_guid":"46231c2a-fef1-4746-9937-d7c50079317d","_uuid":"f99247bf-e9ab-49cf-ac42-e86c4fc4ad61","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:06.028856Z","iopub.execute_input":"2024-08-28T23:40:06.029172Z","iopub.status.idle":"2024-08-28T23:40:06.042911Z","shell.execute_reply.started":"2024-08-28T23:40:06.029140Z","shell.execute_reply":"2024-08-28T23:40:06.042017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef evaluate_model(best_model_path, data_loader, loss_fn, model_name, figure_name, folder_name, device=device):\n\n    # Load the best model\n    model = initialize_model(model_name, n_classes)\n    model.to(device)\n    ckpt = torch.load(best_model_path)\n    model.load_state_dict(ckpt['model_state_dict'])\n    epoch = ckpt['epoch']\n    print(f'Model loaded from the best epoch {epoch}')\n    print('Model loaded')\n    \n    loss_val, acc_val = [], []\n    y_true, y_pred = [], []\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.eval()\n    for x, y in data_loader:\n        x = x.to(device)\n        y = y.to(device).long()\n        \n        with torch.no_grad():\n            # Forward pass\n            out = model(x)\n            l = loss_fn(out, y)\n        \n        predicted_labels = out.argmax(1)\n        y_true.extend(list(y.cpu().numpy()))\n        y_pred.extend(list(predicted_labels.cpu().numpy()))\n        acc = (predicted_labels == y).cpu().numpy().sum() / out.shape[0]\n        loss_val.append(l.cpu().data.numpy())\n        acc_val.append(acc)\n\n    acc_val = np.asarray(acc_val).mean()\n    loss_val = np.asarray(loss_val).mean()\n\n    print('Validation loss: {:.4f}'.format(loss_val))\n    print('Validation accuracy: {:.2f}%'.format(100 * acc_val))\n\n    # Save the model\n    model_filename = os.path.join(folder_name, f'{model_name}.ckpt')\n    torch.save(model.state_dict(), model_filename)\n    print(f'Model saved as {model_name}.ckpt')\n\n    # Print and save classification report\n    cf_report = classification_report(y_true, y_pred, digits=2, target_names=data_loader.dataset.class_names)\n    print(cf_report)\n    cf_report_filename = os.path.join(folder_name, f'{model_name}_classification_report.txt')\n    with open(cf_report_filename, 'w') as f:\n        f.write(cf_report)\n    print(f'Classification report saved as {cf_report_filename}')\n\n    # Compute and plot confusion matrix\n    cmatrix = confusion_matrix(y_true, y_pred)\n    Ncmatrix = cmatrix.astype('float') / cmatrix.sum(axis=1)[:, np.newaxis]\n\n    group_counts = ['{0:0.0f}'.format(value) for value in cmatrix.flatten()]\n    group_percentages = ['{0:.2%}'.format(value) for value in Ncmatrix.flatten()]\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n    labels = np.asarray(labels).reshape(len(data_loader.dataset.class_names), len(data_loader.dataset.class_names))\n\n    # Plot and save the confusion matrix figure\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(Ncmatrix, annot=labels, fmt='', cmap='Reds', xticklabels=data_loader.dataset.class_names, yticklabels=data_loader.dataset.class_names)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\n Acc={:.2f}%'.format(100 * acc_val))\n    \n    plot_name = os.path.join(folder_name, f'{figure_name}.png')\n    plt.savefig(plot_name, bbox_inches='tight', pad_inches=0.2, dpi=500)\n    plt.show()\n    print(f'Figure saved as {figure_name}.png')\n\n    return 100 * acc_val","metadata":{"_cell_guid":"97b6c3f8-0838-4670-8d3a-09be6a3d3804","_uuid":"d615d9ef-8d5f-4c1d-8274-2fca9d6804c2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:06.046412Z","iopub.execute_input":"2024-08-28T23:40:06.046786Z","iopub.status.idle":"2024-08-28T23:40:06.063630Z","shell.execute_reply.started":"2024-08-28T23:40:06.046753Z","shell.execute_reply":"2024-08-28T23:40:06.062694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\ndataset = data_CNN(root_dir, npz_file)\n\n# Split data into train, validation, and test sets\nif val is not None:\n    train_indices, test_indices = train_test_split(list(range(len(dataset))),\n                                                    test_size=test_size, stratify=dataset.y, random_state=random_seed)\n    test_indices, val_indices = train_test_split(test_indices,\n                                                   test_size=val_size, stratify=dataset.y[test_indices], random_state=random_seed)\nelse:\n    train_indices, test_indices = train_test_split(list(range(len(dataset))),\n                                                   test_size=test_size, stratify=dataset.y, random_state=random_seed)\n    val_indices = None\n\n# Calculate class weights\nweights = calculate_class_weights(dataset.y)\nweights = weights.to(device)\nprint(f\"Class weights: {weights}\")\n\n# Create data loaders\nif val:\n    train_loader, val_loader, test_loader = create_data_loaders(dataset, train_indices, val_indices, test_indices, batch_size, val=val)\nelse:\n    train_loader, test_loader = create_data_loaders(dataset, train_indices, val_indices, test_indices, batch_size, val=val)\n    val_loader = None\n\n# Plot class distribution\nplot_class_distribution(train_loader, val_loader, test_loader, n_classes=n_classes)","metadata":{"_cell_guid":"284814fa-53dc-4805-a2b1-3220613a8c90","_uuid":"7b454891-8b7f-4806-826c-81d748f3fc10","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:06.064700Z","iopub.execute_input":"2024-08-28T23:40:06.065012Z","iopub.status.idle":"2024-08-28T23:40:14.170020Z","shell.execute_reply.started":"2024-08-28T23:40:06.064976Z","shell.execute_reply":"2024-08-28T23:40:14.169080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x, y in train_loader:\n    print(x)\n    print(x.shape)\n    print(y.shape)\n    break","metadata":{"_cell_guid":"3ac14202-89ce-4613-b8c7-d0fde4394c80","_uuid":"d9919854-b2ef-4683-9eb6-15e3b68b42ad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:14.171395Z","iopub.execute_input":"2024-08-28T23:40:14.171707Z","iopub.status.idle":"2024-08-28T23:40:14.502234Z","shell.execute_reply.started":"2024-08-28T23:40:14.171674Z","shell.execute_reply":"2024-08-28T23:40:14.501254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif model_name == \"MLP\":\n    folder_name = \"MLP_FR\"\n    os.makedirs(folder_name, exist_ok=True)\n    model = initialize_model(\n        model_name=\"TimeSeriesMLP\",\n        input_size=36,          # 36 time steps\n        output_size=n_classes,  # 10 classes\n        hidden_units=[64, 128], # Example hidden layer sizes\n        dropout_rate=dropout_rate\n    )\n    model.to(device)\n    print(\"Initialized TimeSeriesMLP\")\n\nelif model_name == \"LSTM\":\n    folder_name = \"LSTM_FR\"\n    os.makedirs(folder_name, exist_ok=True)\n    model = initialize_model(\n        model_name=\"TimeSeriesLSTM\",\n        input_size=36,          # 36 features (time steps)\n        output_size=n_classes,  # 10 classes\n        hidden_size=64,         # Example hidden size\n        num_layers=2            # Example number of LSTM layers\n    )\n    model.to(device)\n    print(\"Initialized TimeSeriesLSTM\")\n\nelif model_name == \"CNN\":\n    folder_name = \"CNN_FR\"\n    os.makedirs(folder_name, exist_ok=True)\n    model = initialize_model(\n        model_name=\"CNN1D\",\n        input_size=10,          # 36 features (input channels)\n        n_classes=n_classes,    # 10 classes\n#         dropout_rate=dropout_rate        # Example dropout rate\n    )\n    model.to(device)\n    print(\"Initialized CNN1D\")\n\nelse:\n    raise ValueError(f\"Model {model_name} not recognized\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-28T23:41:33.406442Z","iopub.execute_input":"2024-08-28T23:41:33.406861Z","iopub.status.idle":"2024-08-28T23:41:33.418691Z","shell.execute_reply.started":"2024-08-28T23:41:33.406820Z","shell.execute_reply":"2024-08-28T23:41:33.417679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if loss == \"focal\":\n    loss_fn = FocalLoss(gamma=gamma_loss,weights=weights)\nelse:\n    loss_fn = nn.CrossEntropyLoss(weights=weights)\n#opti = MixOptimizer(model.parameters(),lr=lr)\nopti = optim.Adam(model.parameters(), lr=lr)\nif step_lr:\n    scheduler = lr_scheduler.StepLR(opti, step_size, gamma_value)\nelse:\n    scheduler = lr_scheduler.ExponentialLR(opti, gamma=gamma_value)","metadata":{"_cell_guid":"7784764e-8d29-407d-a739-75f56706198d","_uuid":"ccc9384d-5490-4660-ac24-3191d775b6e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:42:28.331133Z","iopub.execute_input":"2024-08-28T23:42:28.331932Z","iopub.status.idle":"2024-08-28T23:42:28.338249Z","shell.execute_reply.started":"2024-08-28T23:42:28.331892Z","shell.execute_reply":"2024-08-28T23:42:28.337061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nif val:\n    t1, t2, t3, t4, t5, best_epoch, best_model_path = train_model(model, train_loader, val_loader, loss_fn, opti, scheduler, epochs, folder_name)\nelse:\n    t1, t2, t3, t4, t5, best_epoch, best_model_path = train_model(model, train_loader, test_loader, loss_fn, opti, scheduler, epochs, folder_name)","metadata":{"_cell_guid":"dc6ec482-6cc4-4847-9b27-66e0518ada9c","_uuid":"10a9f0ce-336f-4ad7-8ee5-7ba3f3e26c72","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:42:31.685979Z","iopub.execute_input":"2024-08-28T23:42:31.686881Z","iopub.status.idle":"2024-08-28T23:42:32.618201Z","shell.execute_reply.started":"2024-08-28T23:42:31.686838Z","shell.execute_reply":"2024-08-28T23:42:32.616763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplot_training_results(t1, t2, t3, t4, t5,folder_name, plot_figure_name, best_epoch)","metadata":{"_cell_guid":"87177051-5863-4460-8795-870fc013d6af","_uuid":"c7e357a4-01ea-4c5d-976f-54ac59a6e17a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:14.893205Z","iopub.status.idle":"2024-08-28T23:40:14.893568Z","shell.execute_reply.started":"2024-08-28T23:40:14.893388Z","shell.execute_reply":"2024-08-28T23:40:14.893406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nacc_val = evaluate_model(best_model_path, test_loader, loss_fn, model_name, figure_name, folder_name, device=device)","metadata":{"_cell_guid":"fd3b7866-fd3b-492c-ae09-df0fcb6f8af2","_uuid":"451bb825-ef0e-428c-becf-7ea5eaafff84","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-28T23:40:14.894784Z","iopub.status.idle":"2024-08-28T23:40:14.895149Z","shell.execute_reply.started":"2024-08-28T23:40:14.894945Z","shell.execute_reply":"2024-08-28T23:40:14.894961Z"},"trusted":true},"execution_count":null,"outputs":[]}]}